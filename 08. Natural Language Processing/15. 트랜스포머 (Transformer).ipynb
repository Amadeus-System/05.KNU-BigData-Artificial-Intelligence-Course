{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hyoungsun Park - _13 트랜스포머 (Transformer).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6PQSnsGeA2OH"},"source":["# 트랜스포머 (Transformer)\n","\n","* 참고: https://wikidocs.net/31379"]},{"cell_type":"markdown","metadata":{"id":"nbQ-h_XxBAiq"},"source":["* attention mechanism은 seq2seq의 입력 시퀀스 정보 손실을 보정해주기 위해 사용됨\n","* attention mechanism을 보정 목적이 아닌, 인코더와 디코더로 구성한 모델이 바로 트랜스포머\n","* 트랜스포머는 RNN을 사용하지 않고 인코더와 디코더를 설계하였으며, 성능도 RNN보다 우수함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RDiFPIdUBBS2"},"source":["## 포지셔널 인코딩"]},{"cell_type":"markdown","metadata":{"id":"rLqHf_4SEWoa"},"source":["* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n","* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n","* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n","* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산"]},{"cell_type":"code","metadata":{"id":"SiO5c_HIFBAk","executionInfo":{"status":"ok","timestamp":1601393180697,"user_tz":-540,"elapsed":1194,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import numpy as np\n","\n","def positional_encoding(dim, sentence_length):\n","    encoded_vec = np.array([pos / np.power(10000, 2 * i / dim) for pos in range(sentence_length) for i in range(dim)])\n","    encoded_vec[::2] = np.sin(encoded_vec[::2])\n","    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n","    return tf.constant(encoded_vec.reshape([sentence_length, dim]),\n","                       dtype = tf.float32)\n"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"099gUUxhAgy3"},"source":["## 레이어 정규화"]},{"cell_type":"markdown","metadata":{"id":"XCdips98yPuH"},"source":["*  레이어 정규화에서는 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이 값을 통해 값을 정규화함\n","*  해당 정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 `sublayer_connection()`을 선언"]},{"cell_type":"code","metadata":{"id":"TSJjxF86Aeg3","executionInfo":{"status":"ok","timestamp":1601393181940,"user_tz":-540,"elapsed":2409,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def layer_norm(inputs, eps = 1e-6):\n","    feature_shape = inputs.get_shape()[-1:]\n","    mean = tf.keras.backend.mean(inputs, [-1], keepdims = True)\n","    std  = tf.keras.backend.std( inputs, [-1], keepdims = True)\n","    beta = tf.Variable(tf.zeros(feature_shape), trainable = False)\n","    gamma = tf.Variable(tf.ones(feature_shape), trainable = False)\n","    return gamma * (inputs - mean) / (std + eps) + beta # layer normalization\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"km9ORxIun-MU","executionInfo":{"status":"ok","timestamp":1601393181944,"user_tz":-540,"elapsed":2393,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def sublayer_connection(inputs, sublayer, dropout = 0.2):\n","    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n","    return outputs # 레이어 연결해주고 정규화 및 드롭아웃하는 함수"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ppb7IxJ3diMC"},"source":["## 어텐션"]},{"cell_type":"markdown","metadata":{"id":"1JaU6MHgy9V2"},"source":["\n","\n","*   트랜스포머 모델의 핵심이 되는 부분\n","*   트랜스포머에서는 multi-head attention과 self attention이라는 개념을 사용\n","  1.   multi-head attention\n","      * 디코더가 가지는 차원을 나누어 병렬로 어텐션을 진행\n","      *  마지막엔 병렬로 각 진행해 얻은 어텐션 헤드를 모두 연결\n","      * 이로 인해 다양한 시각에서 정보를 수집할 수 있는 효과를 얻음\n","  2.   self attention\n","      *   일반적인 어텐션의 경우, 특정 시점의 디코더 은닉상태와 모든 시점의 인코더 은닉상태를 활용\n","      *   이는 입력 문장과 다른 문장에 존재하는 단어간의 어텐션을 의미함\n","      *   반면 self attention은 은닉 상태를 동일하게 하여 어텐션을 진행\n","      *   이는 입력 문장 내 단어간의 어텐션을 의미함\n","\n","\n","\n","\n","*   트랜스포머 제안 논문에서는 scaled-dot product attention을 활용해 모델을 작성함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kRyL0KDXi6ej"},"source":["### scaled-dot product attention 구현"]},{"cell_type":"markdown","metadata":{"id":"6HtmcgRR3Cr-"},"source":["* scaled-dot product attention은 앞서 학습한 dot product attention과 거의 유사함\n","* 단 attention을 진행할 때 어텐션 스코어를 계산할 때 내적 값을 정규화\n","* 트랜스포머에서는 정규화할 때 K 벡터(=디코더 셀의 은닉 상태)의 차원을 루트를 취한 값을 사용"]},{"cell_type":"code","metadata":{"id":"ALEMzi4fdiSQ","executionInfo":{"status":"ok","timestamp":1601393181947,"user_tz":-540,"elapsed":2368,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def scaled_dot_product_attention(query, key, value, masked = False):\n","    key_dim_size = float(key.get_shape().as_list()[-1])\n","    key = tf.transpose(key, perm = [0, 2, 1])\n","\n","    outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n","\n","    if masked : \n","        diag_vals = tf.ones_like(outputs[0, :, :])\n","        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n","        # 삼각행렬로 나눠 아랫부분을 취함, 나머지 윗부분은 패딩처리\n","        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n","        paddings = tf.ones_like(masks) * (-2 ** 30)\n","        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n","\n","    attention_map = tf.nn.softmax(outputs)\n","    return tf.matmul(attention_map, value)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yr20BxvVi-8b"},"source":["### multi-head attention 구현"]},{"cell_type":"markdown","metadata":{"id":"Gb5qflUH14-H"},"source":["* multi-head attention의 구현 과정\n","  1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성\n","  2. 생성된 행렬들을 heads에 해당하는 수만큼 분리\n","  3. 분리한 행렬들에 대해 각각 어텐션을 수행\n","  4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ooc3FAdQi_Gz","executionInfo":{"status":"ok","timestamp":1601393181948,"user_tz":-540,"elapsed":2349,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def multi_head_attention(query, key, value, num_units, heads, masked = False):\n","    query = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(query)\n","    key = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(key)\n","    value = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(value)\n","\n","    query = tf.concat(tf.split(query, heads, axis = -1), axis = 0)\n","    # 분리하자마자 다시 합침\n","    key = tf.concat(tf.split(key, heads, axis = -1), axis = 0)\n","    value = tf.concat(tf.split(value, heads, axis = -1), axis = 0)\n","    \n","    attention_map = scaled_dot_product_attention(query, key, value, masked)\n","    attn_outputs = tf.concat(tf.split(attention_map, heads, axis = 0), axis = -1)\n","    attn_outputs = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(attn_outputs)\n","\n","    return attn_outputs"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78Zn5-fYITD4"},"source":["## 포지션-와이즈 피드 포워드 신경망"]},{"cell_type":"markdown","metadata":{"id":"-xxeG2xvo3ZN"},"source":["\n","\n","*   multi-head attention의 결과인 행렬을 입력받아 연산\n","*   일반적인 완전 연결 신경망(Dense layer)를 사용\n","*   position-wise FFNN은 인코더와 디코더에 모두 존재\n","\n"]},{"cell_type":"code","metadata":{"id":"0tSFd5OaITJ0","executionInfo":{"status":"ok","timestamp":1601393181949,"user_tz":-540,"elapsed":2334,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def feed_forward(inputs, num_units):\n","    feature_shape = inputs.get_shape()[-1]\n","    inner_layer = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(inputs)\n","    outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n","    return outputs"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuccViYgBK6v"},"source":["## 인코더\n"]},{"cell_type":"markdown","metadata":{"id":"tG3MH0n1JVLz"},"source":["* 인코더는 하나의 어텐션을 사용\n","  + encoder self-attention (multi-head self-attention과 동일)"]},{"cell_type":"code","metadata":{"id":"m5T0pzBoAnn3","executionInfo":{"status":"ok","timestamp":1601393181950,"user_tz":-540,"elapsed":2306,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def encoder_module(inputs, model_dim, ffn_dim, heads):\n","    self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads))\n","    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","    return outputs\n","\n","def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n","    outputs = inputs\n","    for i in range(num_layers):\n","        outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n","\n","    return outputs"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcgHRcTEBQqg"},"source":["## 디코더"]},{"cell_type":"markdown","metadata":{"id":"cNj-6FLQwT4-"},"source":["* 디코더는 다음과 같은 구성의 반복으로 이루어짐\n","  1. masked decoder self-attention\n","  2. encoder-decoder attention\n","  3. position-wise FFNN\n","\n","* 디코더에서는 2종류의 어텐션을 사용\n","  1.   masked decoder self-attention\n","    *   디코더에서는 인코더와는 달리 순차적으로 결과를 만들어 내야하기 때문에 다른 어텐션 방법을 사용함\n","    *   디코더 예측 시점 이후의 위치에 attention을 할 수 없도록 masking 처리\n","    *   결국 예측 시점에서 예측은 미리 알고 있는 위치까지만의 결과에 의존\n","  2.   encoder-decoder attention\n","    *   앞서 설명한 multi-head attention과 동일\n","\n"]},{"cell_type":"code","metadata":{"id":"2B05wr7aARcT","executionInfo":{"status":"ok","timestamp":1601393181951,"user_tz":-540,"elapsed":2290,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n","    masked_self_attn = sublayer_connection(inputs,\n","                                           multi_head_attention(inputs, inputs, inputs,\n","                                                                model_dim, heads, masked = True))\n","    self_attn = sublayer_connection(masked_self_attn,\n","                                    multi_head_attention(masked_self_attn,\n","                                                         encoder_outputs,\n","                                                         encoder_outputs,\n","                                                         model_dim, heads))\n","    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","    return outputs \n","\n","def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n","    outputs = inputs\n","    for i in range(num_layers):\n","        outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n","    \n","    return outputs"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtztlyUB1ERS"},"source":["## 트랜스포머를 활용한 챗봇"]},{"cell_type":"markdown","metadata":{"id":"6CGUIAzv6eWs"},"source":["### konlpy 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"Ae0mHT49v5gy"},"source":["*    한글을 처리하기 위해 konlpy 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"U8yf75uG6hBW","executionInfo":{"status":"ok","timestamp":1601393183617,"user_tz":-540,"elapsed":3935,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"195ed630-919a-49d2-a92d-5b08cd0c8e7f","colab":{"base_uri":"https://localhost:8080/","height":322}},"source":["!pip install konlpy"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n","Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.0.2)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rUMXvK5H1G9H"},"source":["### 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"miXrjR316mNb"},"source":["* 처리에 필요한 각종 변수 선언\n","* filters에 해당되는 문자를 걸러주는 정규 표현식 컴파일\n","\n"]},{"cell_type":"code","metadata":{"id":"SMjn5PfE1GZR","executionInfo":{"status":"ok","timestamp":1601393183619,"user_tz":-540,"elapsed":3908,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import re\n","import tensorflow as tf\n","\n","filters = \"([~.,!?\\\"':;)(])\"\n","PAD = '<PADDING>'\n","STD = '<START>'\n","END = '<END>'\n","UNK = '<UNKNOWN>'\n","\n","PAD_INDEX = 0\n","STD_INDEX = 0\n","END_INDEX = 0\n","UNK_INDEX = 0\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(filters)\n","\n"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmRFuH2r6oNJ"},"source":["* 주소에서 데이터를 가져오는 `load_data()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"CmrmdXkePWYb","executionInfo":{"status":"ok","timestamp":1601395853917,"user_tz":-540,"elapsed":772,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","def load_data(data_path):\n","    data_df = pd.read_csv(data_path, header = 0)\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","    train_input, eval_input, train_label, eval_label = train_test_split(question, answer,\n","                                                                        test_size = 0.33,\n","                                                                        random_state = 111)\n","    return train_input, eval_input, train_label, eval_label"],"execution_count":84,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHuOJHPtPXqq"},"source":["* 처리에 필요한 단어 사전을 생성하는 `load_vocab()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"QtQL-AP06oSa","executionInfo":{"status":"ok","timestamp":1601394420640,"user_tz":-540,"elapsed":796,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def load_vocab(data_path):\n","    data_df = pd.read_csv(data_path, encoding = 'utf-8')\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","    \n","    if tokenize_as_morph:\n","        question = prepro_like_morphlized(question)\n","        answer = prepro_like_morphlized(answer)\n","\n","    data = []\n","    data.extend(question)\n","    data.extend(answer)\n","    words = data_tokenizer(data)\n","    words = list(set(words))\n","    words[:0] = MARKER\n","\n","    char2idx = {char : idx for idx, char in enumerate(words)}\n","    idx2char = {idx : char for idx, char in enumerate(words)}\n","\n","    return char2idx, idx2char, len(char2idx)"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wYtpjv76r5q"},"source":["* 문자열 데이터를 학습에 사용될 수 있도록 변현하는 `prepro_like_morphlized()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"-bQ3FOva6tg6","executionInfo":{"status":"ok","timestamp":1601394421690,"user_tz":-540,"elapsed":634,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["from konlpy.tag import Okt # Open korean text\n","\n","def prepro_like_morphlized(data):\n","    morph_analyzer = Okt()\n","    result_data = list()\n","    for seq in data:\n","        morphlized_seq = ' '.join(morph_analyzer.morphs(seq.replace(' ', '')))\n","        result_data.append(morphlized_seq)\n","    \n","    return result_data"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhsVp4pWPTR3"},"source":["* 단어 사전을 만들기 위해 단어들을 분리하는 `data_tokenizer()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"otLI_RUfPR_g","executionInfo":{"status":"ok","timestamp":1601394423419,"user_tz":-540,"elapsed":873,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def data_tokenizer(data):\n","    words = []\n","    for sentence in data:\n","        sentence = re.sub(CHANGE_FILTER, '', sentence) # 특수기호 필터로 제거\n","        for word in sentence.split():\n","            words.append(word)\n","    return [word for word in words if word]"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OkKPA-Mx6uaC"},"source":["* encoder의 입력을 구성하기 위한 함수 `enc_processing()` 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"jK-yeSThPGsa","executionInfo":{"status":"ok","timestamp":1601394424701,"user_tz":-540,"elapsed":482,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def enc_processing(value, dictionary):\n","    sequences_input_index = []\n","    sequences_length = []\n","\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, '', sequence)\n","        sequence_index = []\n","        for word in sequence.split():\n","            if dictionary.get(word) is not None:\n","                sequence_index.extend([dictionary[word]])\n","            else:\n","                sequence_index.extend([dictionary[UNK]])\n","        if len(sequence_index) > max_len:\n","            sequence_index = sequence_index[:max_len]\n","\n","        sequences_length.append(len(sequence_index))\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_input_index.append(sequence_index)\n","\n","    return np.asarray(sequences_input_index), sequences_length\n","        "],"execution_count":65,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4mM57_FPIg7"},"source":["* decoder의 입력을 구성하기 위한 함수 `dec_output_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"cX_NpcTq6vw6","executionInfo":{"status":"ok","timestamp":1601394426659,"user_tz":-540,"elapsed":828,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def dec_output_processing(value, dictionary):\n","    sequences_output_index = []\n","    sequences_length = []\n","\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, '', sequence)\n","        sequence_index = []\n","        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n","\n","        if len(sequence_index) > max_len:\n","            sequence_index = sequence_index[:max_len]\n","\n","        sequences_length.append(len(sequence_index))\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_output_index.append(sequence_index)\n","\n","    return np.asarray(sequences_output_index), sequences_length\n","        "],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otsTEt4FPLJX"},"source":["* decoder의 출력을 구성하기 위한 함수 `dec_target_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"eeP0PWHEPMma","executionInfo":{"status":"ok","timestamp":1601394428023,"user_tz":-540,"elapsed":784,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def dec_target_processing(value, dictionary):\n","    sequences_target_index = []\n","\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, '', sequence)\n","        sequence_index = [dictionary[word] for word in sequence.split()]\n","\n","        if len(sequence_index) >= max_len:\n","            sequence_index = sequence_index[:max_len - 1] + [dictionary[END]]\n","        else:\n","            sequence_index += [dictionary[END]]\n","\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_target_index.append(sequence_index)\n","\n","    return np.asarray(sequences_target_index)\n","        "],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tb9vVUng6xDq"},"source":["* 모델에 데이터를 효율적으로 투입하도록 `train_input_fn()`, `eval_input_fn()` 함수 선언\n","* `rearrange()`는 dataset 객체가 데이터를 어떻게 변형시킬지 정의해둔 함수\n","* dataset.map은 rearrange 함수를 기반으로 데이터를 변형\n","\n"]},{"cell_type":"code","metadata":{"id":"uAlKV4xF62Uf","executionInfo":{"status":"ok","timestamp":1601395835749,"user_tz":-540,"elapsed":1464,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size):\n","\n","    dataset = tf.compat.v1.data.Dataset.from_tensor_slices((train_input_enc,\n","                                                            train_output_enc,\n","                                                            train_target_dec))\n","    dataset = dataset.shuffle(buffer_size = len(train_input_enc))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(rearrange)\n","    dataset = dataset.repeat(1)\n","\n","    iterator = dataset.make_one_shot_iterator()\n","    return iterator.get_next()\n","\n","\n","def eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size):\n","\n","    dataset = tf.compat.v1.data.Dataset.from_tensor_slices((eval_input_enc, \n","                                                            eval_output_enc,\n","                                                            eval_target_dec))\n","                                                            \n","    dataset = dataset.shuffle(buffer_size = len(eval_input_enc))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(rearrange)\n","    dataset = dataset.repeat()\n","\n","    iterator = dataset.make_one_shot_iterator()\n","    return iterator.get_next()\n","\n","\n","def rearrange(input, output, target):\n","    features = {'input':input, 'output':output}\n","    return features, target\n"],"execution_count":83,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is-GhUDN62xC"},"source":["* 모델의 예측은 배열로 생성되기 때문에 이를 확인하기 위해선 문자열로 변환이 필요\n","* 예측을 문자열로 변환해주는 `pred2string()` 함수 선언\n"]},{"cell_type":"code","metadata":{"id":"jCfwWXhb64Cc","executionInfo":{"status":"ok","timestamp":1601394430945,"user_tz":-540,"elapsed":503,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def pred2string(value, dictionary):\n","    sentence_string = []\n","    is_finished = False # 끝에 있는지 아닌지 유무파악\n","\n","    for v in value:\n","        sentence_string = [dictionary[index] for index in v['indexs']]\n","\n","    answer = \"\"\n","    for word in sentence_string:\n","        if word == END:\n","            if_finished = True\n","            break\n","\n","        if word != PAD and word != END:\n","            answer += word\n","            answer += \" \"\n","\n","    return answer, is_finished"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwp9Nnwz7UoG"},"source":["* 챗봇 데이터 URL: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n","* 데이터 주소에서 데이터를 읽어들여 단어 사전과 사용 데이터 구성"]},{"cell_type":"code","metadata":{"id":"-T536MdU7Taq","executionInfo":{"status":"ok","timestamp":1601394552164,"user_tz":-540,"elapsed":119959,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import pandas as pd\n","\n","tokenize_as_morph = True\n","\n","data_path = 'https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv'\n","\n","char2idx, idx2char, len_vocab = load_vocab(data_path)\n","train_input, eval_input, train_label, eval_label = load_data(data_path)"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gor8TaFjkg1Z","executionInfo":{"status":"ok","timestamp":1601394878094,"user_tz":-540,"elapsed":749,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["train_input = np.array(train_input)\n","eval_input = np.array(eval_input)\n","train_label = np.array(train_label)\n","eval_label = np.array(eval_label)"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Iprg6XdjQUl","executionInfo":{"status":"ok","timestamp":1601394879054,"user_tz":-540,"elapsed":765,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"6b91e233-12de-4dc5-f8c9-0af8033c7a80","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(train_input.shape, train_label.shape)\n","print(eval_input.shape, eval_label.shape)"],"execution_count":73,"outputs":[{"output_type":"stream","text":["(7921,) (7921,)\n","(3902,) (3902,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMwYuLuSmYYw","executionInfo":{"status":"ok","timestamp":1601395355696,"user_tz":-540,"elapsed":1129,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"fdb96afe-94ee-47e8-c48a-0c188d9ea802","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["eval_label[2]"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'많으면 많을 수록 좋죠.'"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"markdown","metadata":{"id":"7cVd7AOKinqn"},"source":["### 모델 구성"]},{"cell_type":"markdown","metadata":{"id":"hqLJ0a6r49yi"},"source":["* 앞서 작성한 트랜스포머 모델을 결합해 학습에 사용할 모델을 구성함"]},{"cell_type":"code","metadata":{"id":"CNeeXoZginvj","executionInfo":{"status":"ok","timestamp":1601394903566,"user_tz":-540,"elapsed":1158,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def model(features, labels, mode, params):\n","    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n","    EVAL = mode == tf.estimator.ModeKeys.EVAL\n","    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n","\n","    position_encode = positional_encoding(params['embedding_size'], params['max_len'])\n","    if params['xavier_initializer']:\n","        embedding_initializer = 'glorot_normal'\n","    else:\n","        embedding_initializer = 'uniform'\n","\n","    embedding = tf.keras.layers.Embedding(params['len_vocab'],\n","                                          params['embedding_size'],\n","                                          embeddings_initializer = embedding_initializer)\n","    \n","    x_embedded_matrix = embedding(features['input']) + position_encode\n","    y_embedded_matrix = embedding(features['output']) + position_encode\n","\n","    encoder_outputs = encoder(x_embedded_matrix, \n","                              params['model_hidden_size'],\n","                              params['ffn_hidden_size'],\n","                              params['attention_head_size'],\n","                              params['layer_size'])\n","    \n","    decoder_outputs = decoder(y_embedded_matrix, encoder_outputs,\n","                              params['model_hidden_size'],\n","                              params['ffn_hidden_size'],\n","                              params['attention_head_size'],\n","                              params['layer_size'])\n","\n","    logits = tf.keras.layers.Dense(params['len_vocab'])(decoder_outputs)\n","    predict = tf.argmax(logits, 2)\n","\n","    if PREDICT:\n","        predictions = {'indexs': predict,\n","                       'logits': logtis}\n","        return tf.estimator.EstimatorSpec(mode, predictions = predictions)\n","\n","    labels_ = tf.one_hot(labels, params['len_vocab'])\n","    loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits = logits,\n","                                                                                labels = labels_))\n","    accuracy = tf.compat.v1.metrics.accuracy(labels = labels, predictions = predict)\n","    \n","    metrics = {'accuracy': accuracy}\n","    tf.summary.scalar('accuracy', accuracy[1])\n","\n","    if EVAL:\n","        return tf.estimator.EstimatorSpec(mode, loss = loss, eval_metrics_ops = metrics) \n","    assert TRAIN\n","\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","    train_op = optimizer.minimize(loss, global_step = tf.compat.v1.train.get_global_step())\n","    return tf.estimator.EstimatorSpec(mode, loss = loss, train_op = train_op)\n","    "],"execution_count":74,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H7PrLEWE1JCs"},"source":["### 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"Gy_Opm_A7DKC"},"source":["*   필요한 각종 인자들을 설정\n","*   인자에 따라 학습 결과가 달라질 수 있기 때문에 세심한 조정이 필요\n"]},{"cell_type":"code","metadata":{"id":"CKGYuqmH6_kj","executionInfo":{"status":"ok","timestamp":1601394904108,"user_tz":-540,"elapsed":591,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["max_len = 25\n","epoch = 5000\n","batch_size = 256\n","embedding_size = 100\n","model_hidden_size = 100\n","ffn_hidden_size = 100\n","attention_head_size = 100\n","\n","lr = 0.001\n","layer_size = 3\n","xavier_initializer = True"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaXalEy57ODq"},"source":["*   앞서 선언한 processing 함수로 데이터를 모델에 투입할 수 있도록 가공\n","*   평가 데이터에도 동일하게 가공"]},{"cell_type":"code","metadata":{"id":"NWlgWWIq1KSh","executionInfo":{"status":"ok","timestamp":1601395090783,"user_tz":-540,"elapsed":186567,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["import numpy as np\n","\n","train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n","train_output_dec, train_output_dec_length = dec_output_processing(train_label, char2idx)\n","train_target_dec = dec_target_processing(train_label, char2idx)\n","\n","eval_input_enc, eval_input_enc_length = enc_processing(eval_input, char2idx)\n","eval_output_dec, eval_output_dec_length = dec_output_processing(eval_label, char2idx)\n","eval_target_dec = dec_target_processing(eval_label, char2idx)\n"],"execution_count":76,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZGgZzWs7Mr7"},"source":["* 앞서 선언한 함수를 통해 모델을 선언하고 학습\n","* `tf.estimator`를 사용해 간편하게 학습 모듈 구성\n"]},{"cell_type":"code","metadata":{"id":"B9vjc3Ck7F4J","executionInfo":{"status":"ok","timestamp":1601395090785,"user_tz":-540,"elapsed":185899,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"e234e14a-ae9a-4a35-c7ea-9cbca99890a7","colab":{"base_uri":"https://localhost:8080/","height":188}},"source":["transformer = tf.estimator.Estimator(\n","    model_fn = model,\n","    params = {'embedding_size' : embedding_size,\n","              'model_hidden_size' : model_hidden_size,\n","              'ffn_hidden_size': ffn_hidden_size,\n","              'attention_head_size': attention_head_size,\n","              'learning_rate': lr,\n","              'len_vocab': len_vocab,\n","              'layer_size': layer_size,\n","              'max_len': max_len,\n","              'xavier_initializer': xavier_initializer}\n",")"],"execution_count":77,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpp0ssqdp6\n","INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpp0ssqdp6', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wl_pwUiw7INZ"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"COO-0PcS7Hy5","executionInfo":{"status":"error","timestamp":1601397203447,"user_tz":-540,"elapsed":28019,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}},"outputId":"ead62e08-3c79-46f3-b8f5-2b7da91546fa","colab":{"base_uri":"https://localhost:8080/","height":673}},"source":["transformer.train(input_fn = lambda : train_input_fn(train_input_enc, \n","                                                     train_output_dec,\n","                                                     train_target_dec,\n","                                                     batch_size), steps = epoch)\n","\n","eval_result = transformer.evaluate(input_fn = lambda : eval_input_fn(eval_input_enc, \n","                                                     eval_output_dec,\n","                                                     eval_target_dec,\n","                                                     batch_size))\n","\n","print(**eval_result)"],"execution_count":90,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmpp0ssqdp6/model.ckpt-217\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 217...\n","INFO:tensorflow:Saving checkpoints for 217 into /tmp/tmpp0ssqdp6/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 217...\n","INFO:tensorflow:loss = 1.390426, step = 217\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 248...\n","INFO:tensorflow:Saving checkpoints for 248 into /tmp/tmpp0ssqdp6/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 248...\n","INFO:tensorflow:Loss for final step: 1.2560942.\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-90-4447f637014c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                      batch_size), steps = epoch)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m eval_result = transformer.evaluate(input_fn = lambda : eval_input_fn(eval_input_enc, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                                      \u001b[0meval_output_dec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                      \u001b[0meval_target_dec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m   def _actual_eval(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    490\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 492\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    493\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    494\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1528\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m     global_step_tensor = tf.compat.v1.train.get_global_step(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[0;34m\"\"\"Call model_fn for evaluation and handle return values.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m     features, labels, input_hooks = self._get_features_and_labels_from_input_fn(\n\u001b[0;32m-> 1561\u001b[0;31m         input_fn, ModeKeys.EVAL)\n\u001b[0m\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m     estimator_spec = self._call_model_fn(features, labels, ModeKeys.EVAL,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1037\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-90-4447f637014c>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                      \u001b[0meval_output_dec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                      \u001b[0meval_target_dec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                      batch_size))\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-83-978bd4978c85>\u001b[0m in \u001b[0;36meval_input_fn\u001b[0;34m(eval_input_enc, eval_output_enc, eval_target_dec, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   2507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 2509\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   2510\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m       return DatasetV1Adapter(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4046\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4047\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2939\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3363\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__rearrange() missing 1 required positional argument: 'target'\n"]}]},{"cell_type":"code","metadata":{"id":"WBW6Pvfye5te","executionInfo":{"status":"aborted","timestamp":1601395120715,"user_tz":-540,"elapsed":214780,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["    print(**eval_result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNcrVf2z1LSM"},"source":["### 예측"]},{"cell_type":"markdown","metadata":{"id":"R5lY9DrW8eSK"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"N9IQaBx4Qw8J","executionInfo":{"status":"aborted","timestamp":1601393474792,"user_tz":-540,"elapsed":294746,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["def chatbor(question):\n","    pred_input_enc, pred_input_enc_length = enc_processing([question], char2idx)\n","    pred_output_dec, pred_output_dec_length = dec_output_processing([''], char2idx)\n","    pred_target_dec = dec_target_processing([''], char2idx)\n","\n","    for i in range(max_len):\n","        if i > 0:\n","            pred_output_dec, pred_output_dec_length = dec_output_processing([answer], char2idx)\n","            pred_target_dec = dec_target_processing([answer], char2idx)\n","        predictions = transformer.predict(input_fn = \n","                                          lambda: eval_input_fn(pred_input_enc, pred_output_dec, pred_target_dec, 1))\n","        answer, finished = pred2string(predictions, idx2char)\n","\n","        if finished :\n","            break\n","    \n","    return answer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjHZKvJ31MAU","executionInfo":{"status":"aborted","timestamp":1601393474795,"user_tz":-540,"elapsed":294730,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["chatbor('안녕?')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mjRZwyLQ_gP","executionInfo":{"status":"aborted","timestamp":1601393474797,"user_tz":-540,"elapsed":294715,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["chatbor('너 누구냐?')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7AJCsXRTqJx","executionInfo":{"status":"aborted","timestamp":1601393474799,"user_tz":-540,"elapsed":294697,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["chatbor('뭐 먹었어?')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_M8mfoUfeAWQ","executionInfo":{"status":"aborted","timestamp":1601393474800,"user_tz":-540,"elapsed":294680,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["chatbor('놀고 싶다?')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5mrdGRaem6v","executionInfo":{"status":"aborted","timestamp":1601393474801,"user_tz":-540,"elapsed":294659,"user":{"displayName":"Hyoungsun Park","photoUrl":"","userId":"14179648673617368098"}}},"source":["chatbor('이제 잘래.')"],"execution_count":null,"outputs":[]}]}